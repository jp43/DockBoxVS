#!/usr/bin/env python
import os
import sys
import shutil
import argparse

import ConfigParser
from glob import glob

import pandas as pd

from dockbox_vs import queuing
from dockbox_vs import utils

parser = argparse.ArgumentParser(description="Build directories and config files for Virtual Screening (4th stage)")

parser.add_argument('-l',
    type=str,
    dest='input_files_l',
    nargs='+',
    metavar='FILE',
    default=['compounds.csv'],
    help='ligand file(s): .mol2, .csv (default: compounds.csv)')

parser.add_argument('-r',
    type=str,
    dest='input_files_r',
    nargs='+',
    metavar='FILE',
    default=['targets.csv'], 
    help = 'target file(s): .pdb, .csv (default: targets.csv)')

parser.add_argument('-f',
    type=str,
    dest='config_file',
    metavar='FILE',
    default='config.ini',
    help='config file: .ini')

parser.add_argument('-cutoff',
    dest='cutoff',
    type=float,
    metavar='RMSD_VALUE',
    default=2.0,
    help='RMSD cutoff used for consensus docking or score-based consensus docking. Default: 2.0 A')

parser.add_argument('-level',
    dest='level',
    type=int,
    metavar='INT',
    required=True,
    help='Level of Virtual Screening considered. From 0 (docking with a few compounds) to 3 (VS with a large number of compounds)')

parser.add_argument('-nligands-per-bin',
    dest='nligands_per_bin',
    type=int,
    metavar='INT',
    default=100,
    help='Number of ligands to be included in a subfolder (default: 100)')

parser.add_argument('-nligands-per-job',
    dest='nligands_per_job',
    type=int,
    metavar='INT',
    default=None,
    help='Number of ligands to be run for every submitted job (used for VS levels 2 or 3))')

parser.add_argument('-s',
    dest='sitecsv',
    type=str,
    metavar='FILE',
    default='sites.csv',
    help='csvfile with binding sites: .csv (default: sites.csv)')

parser.add_argument('-w',
    dest='rundir',
    type=str,
    default='vs',
    metavar='DIRECTORY NAME',
    help='name of directory created for virtual screening')

parser.add_argument('-sf',
    dest='sf',
    nargs='+',
    action='append',
    metavar='FUNC',
    help='Scoring functions used to extract the best pose (combination of scores)')

parser.add_argument('-cd',
    dest='cd',
    nargs='+',
    action='append',
    metavar='PRGM',
    help='Docking programs used for standard consensus docking')

parser.add_argument('-sbcd',
    dest='sbcd',
    nargs='+',
    action='append',
    metavar='FUNC',
    help='Scoring functions used for score-based consensus docking')

group = parser.add_mutually_exclusive_group(required=False)

for sch in queuing.known_schedulers:
    group.add_argument('-%s'%sch,
        dest='%s_options'%sch,
        type=str,
        metavar='OPTIONS',
        default=None,
        help='Options for %s'%queuing.known_schedulers[sch])

args = parser.parse_args()

if args.level in [2, 3]:
    if args.nligands_per_job is None:
        raise ValueError('-nligands-per-job option should be specified with VS lvl 2 or 3!')
    else:
        nligands_per_job = args.nligands_per_job

scheduler = None
exe = None
for sch in queuing.known_schedulers:
    args_dict = vars(args)
    if args_dict[sch+'_options'] is not None:
        scheduler = sch
        exe = queuing.exes[sch]
        scheduler_options = queuing.check_scheduler_options(args_dict[sch+'_options'], scheduler)
if scheduler is None:
    scheduler_options = None

if scheduler is None and args.level in [1, 2, 3]:
    sys.exit('Info about the scheduler should be provided when VS level is 1 or more!')

if args.level in [2, 3]:
    if args.nligands_per_job is None:
        sys.exit('nligands_per_job option should be specified when VS level is 2 or 3!')
    else:
        nligands_per_job = args.nligands_per_job

locals().update(args.__dict__)
exts = list(set([os.path.splitext(ff)[1] for ff in args.input_files_r]))
if len(exts) != 1: # if more than one extension provided
    sys.exit("All files specified with -r option must have the same extension!")

# check target input files
input_files_r = []
if exts[0] == '.pdb': # if input files are pdbfiles
    for file_r in args.input_files_r:
        if os.path.exists(file_r):
            input_files_r.append(os.path.abspath(file_r))
        else:
            raise ValueError("File %s not found!"%(file_r))

    ntargets = len(input_files_r)
    nid_digits = max(3, len(str(ntargets)))
    targetids = []
    for jdx, file_r in enumerate(input_files_r):
        targetids.append('target'+(nid_digits-len(str(jdx+1)))*'0' + str(jdx+1))
    csvfile_r = None
    is_csvfile_r = False
elif exts[0] == '.csv': # if input files is the csvfile
    if len(args.input_files_r) != 1:
        raise ValueError("More than 1 csvfile specified with -r option.")

    csvfile_r = os.path.abspath(args.input_files_r[0])
    df_targets = pd.read_csv(csvfile_r)

    input_files_r = [os.path.abspath(ff) for ff in list(df_targets['pdbfile'])]
    ntargets = len(input_files_r)
    targetids = list(df_targets['targetID'])
    is_csvfile_r = True
else:
    raise IOError("Extension of files used with -r option not recognized!")

if ntargets == 1:
    use_target_folder = False
else:
    use_target_folder = True

exts_l = list(set([os.path.splitext(ff)[1] for ff in args.input_files_l]))
if len(exts_l) != 1: # if more than one extension provided
    raise ValueError("All files specified with -l option must have the same extension!")

# check ligand input files
input_files_l = []
if exts_l[0] == '.mol2': # if input files are .mol2
    for file_l in args.input_files_l:
        if os.path.exists(file_l):
            input_files_l.append(os.path.abspath(file_l))
        else:
            raise ValueError("File %s not found!"%(file_l))

    nligands = len(input_files_l)
    nid_digits = max(3, len(str(nligands)))
    ligandids = []

    for jdx, file_r in enumerate(input_files_l): 
        nligands_file_l = utils.get_number_of_compounds(file_l)
        if nligands_file_l == 1:
            ligandids.append('lig'+(nid_digits-len(str(jdx+1)))*'0' + str(jdx+1))
        else:
            raise IOError("Every ligand file should contain at least 1 structure, or use the prepare_compounds routine!")
    csvfile_l = None
    is_csvfile_l = False
    use_isomer_folder = False

elif exts_l[0] == '.csv': # if input files is a csvfile
    if len(args.input_files_l) != 1:
        raise ValueError("More than 1 csvfile specified with -l option.")

    csvfile_l = os.path.abspath(args.input_files_l[0])
    df_ligands = pd.read_csv(csvfile_l)

    input_files_l = [os.path.abspath(ff) for ff in list(df_ligands['mol2file'])]
    nligands = len(input_files_l)
    ligandids = list(df_ligands['ligID'])
    isomers = list(df_ligands['isomer'])

    is_csvfile_l = True
    if all(x==1 for x in isomers):
        use_isomer_folder = False
    else:
        use_isomer_folder = True
else:
    raise IOError("Extension of files used with -r option not recognized!")

nligands_per_bin = args.nligands_per_bin
# check if the number of ligands is large enough to consider subfolders
if nligands > nligands_per_bin:
    use_subfolders = True
else:
    use_subfolders = False

if not os.path.isfile(args.config_file):
    raise ValueError("Config file %s not found!"%args.config_file)

config = ConfigParser.SafeConfigParser()
config.read(args.config_file)

# get names of docking programs from config file
if config.has_option('DOCKING', 'program'):
    instances_config = config.get('DOCKING', 'program').lower()
    instances_config = map(str.strip, instances_config.split(','))
else:
    instances_config = None
# get names of scoring functions from config file
if config.has_option('RESCORING', 'program'):
    scoring_functions_config = config.get('RESCORING', 'program').lower()
    scoring_functions_config = map(str.strip, scoring_functions_config.split(','))
else:
    scoring_functions_config = None

extraction_methods = [('SBCD',args.sbcd), ('CD',args.cd), ('SF',args.sf)]

# check if scoring functions
for method, scoring_functions_list in extraction_methods:
    if scoring_functions_list:
        for scoring_functions in scoring_functions_list:
            for sf in scoring_functions:
                if method in ['SBCD', 'SF'] and (sf not in scoring_functions_config) and not (sf == 'score' and method == 'SF'):
                    sys.exit("Scoring function %s needed with %s not specified in the config file!"%(sf, method))
                elif method == 'CD' and sf not in instances_config:
                    sys.exit("Docking instance %s needed with %s not specified in the config file!"%(sf, method))

if args.level == 3:
    if not is_csvfile_r:
        sys.exit("Protein information should be provided as a .csv file with VS level 3!")

    extract_dirs = '$dir_l'
    if use_target_folder:
        create_target_line = '\n    workdir=$dir_l/${targetids[$id_r]}\n    mkdir $workdir'
        extract_dirs += '/target*'
    else:
        create_target_line = '\n    workdir=$dir_l'

    create_isomerdir_line = ''
    if use_isomer_folder:
        create_isomerdir_line = '\n    workdir=$workdir/isomer${isomerids[$id_l]}\n    mkdir $workdir'
        extract_dirs += '/isomer${isomerids[$id_l]}'

    if csvfile_l:
        csvfile_l_rel = os.path.relpath(csvfile_l, rundir)
    csvfile_r_rel = os.path.relpath(csvfile_r, rundir)

def update_config_file(new_config_file, config_file, label_r, csvfile):
    """Update binding site parameters in config file"""

    # create tmp config file name from original config file
    tmp_config_file = list(os.path.splitext(new_config_file))
    tmp_config_file.insert(1,'_tmp')
    tmp_config_file = ''.join(tmp_config_file)

    # remove section 'SITE' and option site in DOCKING section of config file if exists
    with open(tmp_config_file, 'w') as tmpf:
        with open(config_file, 'r') as newf:
            isdock = False
            sitesection = False
            docksection = False
            for line in newf:
                # check if still in section SITE*
                if line.startswith('[SITE'):
                    sitesection = True
                if sitesection and line.startswith('[') and not line.startswith('[SITE'): # new section has been reached
                    sitesection = False
                # check if still in section DOCKING
                if line.startswith('[DOCKING]'):
                    docksection = True
                    isdock = True
                if docksection and line.startswith('[') and not line.startswith('[DOCKING]'): # new section has been reached
                    docksection = False
                # check if option line in section DOCKING
                if line.strip().startswith('site') and docksection:
                    siteline = True
                else:
                    siteline = False
                if not sitesection and not siteline:
                    newline = line.replace("$targetid", label_r)
                    newline = newline.replace("${targetid}", label_r)
                    newline = newline.replace("${target_id}", label_r)
                    newline = newline.replace("$target_id", label_r)
                    tmpf.write(newline)
    shutil.move(tmp_config_file, new_config_file)

    df = pd.read_csv(csvfile)
    rows = df[df['target'] == label_r]
    if rows.empty:
        sys.exit("No information regarding the site of target %s in .csv file"%label_r)

    nsites = len(rows)
    if nsites == 1:
         # add new sections 'SITE' and option site
        with open(tmp_config_file, 'w') as tmpf:
            with open(new_config_file, 'r') as newf:
                for line in newf:
                    tmpf.write(line)
                for row in rows.iterrows():
                    section = 'SITE'
                    center_conf = row[1]['center']
                    boxsize_conf = row[1]['size']

                    newsite_section = """
[%(section)s]
center = %(center_conf)s
boxsize = %(boxsize_conf)s"""% locals()
                    tmpf.write(newsite_section+'\n')
    elif nsites > 1:
        # add new sections 'SITE' and option site
        with open(tmp_config_file, 'w') as tmpf:
            with open(new_config_file, 'r') as newf:
                for line in newf:
                    tmpf.write(line)
                    if line.startswith('[DOCKING]'):
                        tmpf.write('site = ' + ', '.join(['site%s'%int(row[1]['site']) for row in rows.iterrows()])+'\n')
                for row in rows.iterrows():
                    section = 'SITE' + str(int(row[1]['site']))
                    center_conf = row[1]['center']
                    boxsize_conf = row[1]['size']

                    newsite_section = """
[%(section)s]
center = %(center_conf)s
boxsize = %(boxsize_conf)s"""% locals()
                    tmpf.write(newsite_section+'\n')
    shutil.move(tmp_config_file, new_config_file)

rundir = args.rundir
# always overwrite by default
shutil.rmtree(rundir, ignore_errors=True)
os.mkdir(rundir)

config_file_basename = os.path.basename(args.config_file)

# copy config files
config_suff, config_ext = os.path.splitext(config_file_basename)

configdir = config_suff
shutil.rmtree(configdir, ignore_errors=True)
os.mkdir(configdir)

config_files = []
for idx, file_r in enumerate(input_files_r):
    recid = targetids[idx]
    new_config_file = configdir + '/' + config_suff + '_%s'%(idx+1) + config_ext

    update_config_file(new_config_file, args.config_file, recid, args.sitecsv)
    config_files.append(new_config_file)

subdirs = []
for jdx in range(nligands):
    ligid = ligandids[jdx]
    subdir = rundir
    if use_subfolders:
        minbin = (int(ligid[3:])-1)/nligands_per_bin
        minbin = minbin*nligands_per_bin + 1

        maxbin = min(minbin + nligands_per_bin - 1, nligands)

        minbin_str = str(minbin)
        minbin_str = (len(ligid[3:])-len(minbin_str))*'0'+minbin_str

        maxbin_str = str(maxbin)
        maxbin_str = (len(ligid[3:])-len(maxbin_str))*'0'+maxbin_str

        subdir += '/lig' + minbin_str + '-' + maxbin_str
    subdirs.append(subdir)

    for idx in range(ntargets):
        recid = targetids[idx]
        if use_isomer_folder and use_target_folder:
            workdir = subdir + '/' + ligid + '/' + recid + '/isomer' + str(isomers[jdx])
        elif use_isomer_folder:
            workdir = subdir + '/' + ligid + '/isomer' + str(isomers[jdx])
        elif use_target_folder:
            workdir = subdir + '/' + ligid + '/' + recid
        else:
            workdir = subdir + '/' + ligid

        if level is None or level in [0, 1, 2]:
            os.makedirs(workdir)

        # create list of relative paths for targets and receptors
        if jdx == 0 and idx == 0:
            input_files_l_rel = []
            for file_l in input_files_l:
                input_files_l_rel.append(os.path.relpath(file_l, workdir))

            input_files_r_rel = []
            for file_r in input_files_r:
                input_files_r_rel.append(os.path.relpath(file_r, workdir))

            config_files_rel = []
            for file_c in config_files:
                config_files_rel.append(os.path.relpath(file_c, workdir))

        if level is None or level in [0, 1, 2]:
            script = ""
            if level == 0:
                filename = workdir+"/run."+scheduler
                script += queuing.make_header(scheduler_options, scheduler)
            else:
                filename = workdir+"/run.sh"
                script += '#!/bin/bash'
            script += "\n\nrundbx -f %s -l %s -r %s\n"%(config_files_rel[idx], input_files_l_rel[jdx], input_files_r_rel[idx])

            with open(filename, 'w') as ff:
                ff.write(script)

if level is not None:
    scriptdir = 'to_submit_' + rundir
    shutil.rmtree(scriptdir, ignore_errors=True)
    os.mkdir(scriptdir)
    submit_all_filename = scriptdir + '/submit_all.sh'

if level == 0:
    dirs = rundir
    if use_subfolders:
       dirs += '/from_*'
    dirs += '/lig*'
    if use_target_folder:
        dirs += '/target*'
    if use_isomer_folder:
        dirs += '/isomer*'
    # write script to submit all jobs
    script_all = """#!/bin/bash
set -e
curdir=`pwd`
for dir in %(dirs)s; do
  cd $dir
  %(exe)s run.%(scheduler)s
  cd $curdir
done\n"""%locals()
    with open(submit_all_filename, 'w') as ff:
        ff.write(script_all)

elif level == 1:
    for jdx in range(nligands):
        ligid = str(ligandids[jdx])
        dirs = subdirs[jdx] + '/' + ligid
        if use_target_folder:
            dirs += '/target*'
        if use_isomer_folder:
            dirs += '/isomer' + str(isomers[jdx])
        script = queuing.make_header(scheduler_options, scheduler, jobname=ligid)
        script += """\nset -e
curdir=`pwd`
for dir in %(dirs)s; do
  cd $dir
  bash run.sh
  cd $curdir
done\n"""%locals()
        with open(scriptdir+'/run_vs_%i.'%(jdx+1)+scheduler, 'w') as ff:
            ff.write(script)

    with open(submit_all_filename, 'w') as ff:
        ff.write("""#!/bin/bash
set -e
curdir=`pwd`
for file in %(scriptdir)s/run_vs_*.%(scheduler)s; do
  %(exe)s $file
done\n"""%locals())

elif level in [2, 3]:
    # create jobs to be submitted
    nscripts = nligands/nligands_per_job

    for idx in range(nscripts+1):
        if idx < nscripts:
            idx_first = idx*nligands_per_job
            idx_last = (idx+1)*nligands_per_job-1
        elif idx == nscripts:
            nligands_last_job = nligands - nscripts*nligands_per_job
            if nligands_last_job != 0:
                idx_first = nscripts*nligands_per_job
                idx_last = nscripts*nligands_per_job + nligands_last_job - 1
            else:
                break
        index = idx + 1
        script = queuing.make_header(scheduler_options, scheduler, jobname="vs%s"%index, output=scheduler+"-vs-%s.out"%index, error=scheduler+"-vs-%s.err"%index)

        if level == 2:
            workdirs = []
            for jdx in range(idx_first, idx_last+1):
                dir = subdirs[jdx] + '/' + ligandids[jdx]
                if use_target_folder:
                    dir += '/target*'
                if use_isomer_folder:
                    dir += '/isomer' + str(isomers[jdx])
                workdirs.append(dir)

            workdirs_str = ' '.join(workdirs)
            script += """\nset -e
dirs=`echo %(workdirs_str)s`
curdir=`pwd`
for dir in $dirs; do
  cd $dir
  bash run.sh
  cd $curdir
done\n"""%locals()
        elif level == 3:
            ligandids_str = " ".join(["'%s'"%id for id in ligandids[idx_first:idx_last+1]])
            targetids_str = " ".join(["'%s'"%id for id in sorted(list(set(targetids)))])
            isomerids_str = ' '.join(["'%s'"%isomers[jdx] for jdx in range(idx_first,idx_last+1)])

            files_l_str = " ".join(["'%s'"%ff for ff in input_files_l_rel[idx_first:idx_last+1]])
            files_r_str = " ".join(["'%s'"%ff for ff in input_files_r_rel])
            files_c_str = " ".join(["'%s'"%ff for ff in config_files_rel])

            extract_lines = ''
            for method, scoring_functions_list in extraction_methods:
                if scoring_functions_list:
                    for scoring_functions in scoring_functions_list:
                        method_lower = method.lower()
                        results_dir = '${dir_l}_%s_%s'%(method_lower,'_'.join(scoring_functions))
                        extract_lines += '\n\n  extract_dbx_best_poses -dirs %s -%s %s -all-targets -cutoff %s -r %s -csvr %s'\
%(extract_dirs,method_lower,' '.join(scoring_functions),args.cutoff,results_dir,csvfile_r_rel) 
                        if csvfile_l:
                            extract_lines += " -csvl %s"%csvfile_l_rel
                        # line to move best poses if they exists
                        extract_lines += '\n  if [ -d %s/$dir_l ]; then mv %s/$dir_l ${dir_l}_tmp/%s_%s; fi'%(results_dir,results_dir,method_lower,'_'.join(scoring_functions))
                        # line to move best poses if they exists
                        extract_lines += '\n  tail -n $lines_to_save %s/best_poses.csv >> %s_%s_%s.csv '%(results_dir,method_lower,'_'.join(scoring_functions),index)
                        extract_lines += '\n  rm -rf %s'%results_dir

            script += """\nset -e
# get ids
declare -a ligids=(%(ligandids_str)s)
declare -a targetids=(%(targetids_str)s)
declare -a isomerids=(%(isomerids_str)s)

# get relative files locations
declare -a files_l=(%(files_l_str)s)
declare -a files_r=(%(files_r_str)s)
declare -a files_c=(%(files_c_str)s)

nfiles_l=`echo ${#files_l[@]}`
nfiles_r=`echo ${#files_r[@]}`

cd %(rundir)s
rm -rf sbcd_*_%(index)s.csv cd_*_%(index)s.csv sf_*_%(index)s.csv

for id_l in `seq 0 $((nfiles_l-1))`; do
  curdir=`pwd`
  # find ligand ID
  dir_l=${ligids[$id_l]}

  # make directory
  rm -rf $dir_l
  mkdir $dir_l

  if [ $id_l -eq 0 ]; then lines_to_save=2; else lines_to_save=1; fi

  for id_r in `seq 0 $((nfiles_r-1))`; do%(create_target_line)s%(create_isomerdir_line)s
    cd $workdir
    # run dockbox command
    rundbx -f ${files_c[$id_r]} -l ${files_l[$id_l]} -r ${files_r[$id_r]} 1> /dev/null
    # collect results
    cd $curdir
  done

  # extract results
  mkdir ${dir_l}_tmp%(extract_lines)s

  rm -rf $dir_l
  if [ -z "$(ls -A ${dir_l}_tmp)" ]; then rm -rf ${dir_l}_tmp; else mv ${dir_l}_tmp ${dir_l}; fi
done\n"""%locals()

        with open(scriptdir+'/run_vs_%i.'%index+scheduler, 'w') as ff:
            ff.write(script)

    with open(submit_all_filename, 'w') as ff:
        ff.write("""#!/bin/bash
set -e
for file in %(scriptdir)s/run_vs_*.%(scheduler)s; do
  %(exe)s $file
done"""%locals())
