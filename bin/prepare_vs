#!/usr/bin/env python
import os
import sys
import shutil
import argparse
import subprocess

import ConfigParser
import pandas as pd

from dockbox_vs import queuing
from dockbox_vs import utils

parser = argparse.ArgumentParser(description="Build directories and config files for Virtual Screening (4th stage)")

parser.add_argument('-l',
    type=str,
    dest='input_file_l',
    metavar='FILE',
    default='compounds.csv',
    help='ligand file: .csv (default: compounds.csv)')

parser.add_argument('-r',
    type=str,
    dest='input_files_r',
    nargs='+',
    metavar='FILE',
    default=['targets.csv'], 
    help = 'target file(s): .pdb, .csv (default: targets.csv)')

parser.add_argument('-f',
    type=str,
    dest='config_file',
    metavar='FILE',
    default='config.ini',
    help='config file: .ini')

parser.add_argument('-level',
    dest='level',
    type=int,
    metavar='INT',
    choices=range(2),
    default=None,
    help='Level of screening considered. 0: few compounds; 1: large number of compounds.')

parser.add_argument('-nfolders',
    dest='nfolders_per_layer',
    type=int,
    metavar='INT',
    default=100,
    help='Number of folders in a layer (default: 100)')

parser.add_argument('-nligands-per-job',
    dest='nligands_per_job',
    type=int,
    metavar='INT',
    default=None,
    help='Number of ligands to be run for every submitted job (used for VS level 1))')

parser.add_argument('-rescore-only',
    dest='rescore_only',
    action='store_true',
    default=False,
    help='Rescore only with DockBox!')

parser.add_argument('-s',
    dest='sitecsv',
    type=str,
    metavar='FILE',
    default='sites.csv',
    help='csvfile with binding sites: .csv (default: sites.csv)')

parser.add_argument('-w',
    dest='rundir',
    type=str,
    default='vs',
    metavar='DIRECTORY NAME',
    help='name of directory created for virtual screening')

group = parser.add_mutually_exclusive_group(required=False)
for sch in queuing.known_schedulers:
    group.add_argument('-%s'%sch,
        dest='%s_options'%sch,
        type=str,
        metavar='OPTIONS',
        default=None,
        help='Options for %s'%queuing.known_schedulers[sch])

args = parser.parse_args()

scheduler = None
exe = None
for sch in queuing.known_schedulers:
    args_dict = vars(args)
    if args_dict[sch+'_options'] is not None:
        scheduler = sch
        exe = queuing.exes[sch]
        scheduler_options = queuing.check_scheduler_options(args_dict[sch+'_options'], scheduler)

if scheduler is None:
    scheduler_options = None
    if args.level in [0, 1]:
        sys.exit('Info about the scheduler should be provided when level option is specified!')

if args.level == 1:
    if args.nligands_per_job is None:
        sys.exit('The -nligands-per-job option should be specified when level is 1!')

locals().update(args.__dict__)
exts = list(set([os.path.splitext(ff)[1] for ff in input_files_r]))
if len(exts) != 1: # if more than one extension provided
    sys.exit("All files specified with -r option must have the same extension!")

# check target input files
input_files_r = []
if exts[0] == '.pdb': # if input files are pdbfiles
    for file_r in args.input_files_r:
        if os.path.exists(file_r):
            input_files_r.append(os.path.abspath(file_r))
        else:
            raise ValueError("File %s not found!"%(file_r))

    ntargets = len(input_files_r)
    nid_digits = max(3, len(str(ntargets)))
    targetids = []

    for jdx, file_r in enumerate(input_files_r):
        targetids.append('target'+(nid_digits-len(str(jdx+1)))*'0' + str(jdx+1))
    csvfile_r = None
    is_csvfile_r = False

elif exts[0] == '.csv': # if input files is the csvfile
    if len(args.input_files_r) != 1:
        raise ValueError("More than 1 csvfile specified with -r option.")
    csvfile_r = os.path.abspath(args.input_files_r[0])
    df_targets = pd.read_csv(csvfile_r)

    input_files_r = [os.path.abspath(ff) for ff in list(df_targets['pdbfile'])]
    ntargets = len(input_files_r)
    targetids = list(df_targets['targetID'])
    is_csvfile_r = True

else:
    raise ValueError("target files should be in csv or pdb formats!")
use_target_folder = True

# check ligand input files
suffix, ext = os.path.splitext(args.input_file_l)
if ext == '.csv': # if input files is a csvfile
    csvfile_l = os.path.abspath(args.input_file_l)
    nligands = int(subprocess.check_output("wc -l %s"%csvfile_l, shell=True).split()[0])-1
    csv_iterator = pd.read_csv(csvfile_l, low_memory=False, iterator=True, chunksize=10000)
else:
    raise ValueError("Ligand file should be in csv format!")

runscript_suffix = 'run'
# always overwrite by default
shutil.rmtree(rundir, ignore_errors=True)
os.mkdir(rundir)

nlayers = 0
nfolders_layer = nligands
nfolders_per_layer = args.nfolders_per_layer

# check how many folder layers are needed
while nfolders_layer/nfolders_per_layer > 1: #and nfolders_layer >= 500:
    nlayers += 1
    nfolders_layer_new = nfolders_layer/nfolders_per_layer

    if nfolders_layer%nfolders_per_layer > 0:
        nfolders_layer_new += 1
    nfolders_layer = nfolders_layer_new

if not os.path.isfile(config_file):
    raise ValueError("Config file %s not found!"%config_file)

def update_config_file(new_config_file, old_config_file, label_r, csvfile):
    """Update binding site parameters in config file"""

    # create tmp config file name from original config file
    tmp_config_file = list(os.path.splitext(new_config_file))
    tmp_config_file.insert(1,'_tmp')
    tmp_config_file = ''.join(tmp_config_file)

    # remove section 'SITE' and option site in DOCKING section of config file if exists
    with open(tmp_config_file, 'w') as tmpf:
        with open(old_config_file, 'r') as oldf:
            isdock = False
            sitesection = False
            docksection = False
            for line in oldf:
                # check if still in section SITE*
                if line.startswith('[SITE'):
                    sitesection = True
                if sitesection and line.startswith('[') and not line.startswith('[SITE'): # new section has been reached
                    sitesection = False
                # check if still in section DOCKING
                if line.startswith('[DOCKING]'):
                    docksection = True
                    isdock = True
                if docksection and line.startswith('[') and not line.startswith('[DOCKING]'): # new section has been reached
                    docksection = False
                # check if option line in section DOCKING
                if line.strip().startswith('site') and docksection:
                    siteline = True
                else:
                    siteline = False
                if not sitesection and not siteline:
                    newline = line.replace("$targetid", label_r)
                    newline = newline.replace("${targetid}", label_r)
                    newline = newline.replace("${target_id}", label_r)
                    newline = newline.replace("$target_id", label_r)
                    tmpf.write(newline)
    shutil.move(tmp_config_file, new_config_file)

    df = pd.read_csv(csvfile)
    rows = df[df['target'] == label_r]
    if rows.empty:
        sys.exit("No information regarding the site of target %s in .csv file"%label_r)

    nsites = len(rows)
    if nsites == 1:
         # add new sections 'SITE' and option site
        with open(tmp_config_file, 'w') as tmpf:
            with open(new_config_file, 'r') as newf:
                for line in newf:
                    tmpf.write(line)
                for row in rows.iterrows():
                    section = 'SITE'
                    center_conf = row[1]['center']
                    boxsize_conf = row[1]['size']

                    newsite_section = """
[%(section)s]
center = %(center_conf)s
boxsize = %(boxsize_conf)s"""% locals()
                    tmpf.write(newsite_section+'\n')
    elif nsites > 1:
        # add new sections 'SITE' and option site
        with open(tmp_config_file, 'w') as tmpf:
            with open(new_config_file, 'r') as newf:
                for line in newf:
                    tmpf.write(line)
                    if line.startswith('[DOCKING]'):
                        tmpf.write('site = ' + ', '.join(['site%s'%int(row[1]['site']) for row in rows.iterrows()])+'\n')
                for row in rows.iterrows():
                    section = 'SITE' + str(int(row[1]['site']))
                    center_conf = row[1]['center']
                    boxsize_conf = row[1]['size']

                    newsite_section = """
[%(section)s]
center = %(center_conf)s
boxsize = %(boxsize_conf)s"""% locals()
                    tmpf.write(newsite_section+'\n')
    shutil.move(tmp_config_file, new_config_file)

config_file_basename = os.path.basename(config_file)
config_suff, config_ext = os.path.splitext(config_file_basename)

configdir = config_suff
shutil.rmtree(configdir, ignore_errors=True)
os.mkdir(configdir)

config_files = []
for idx, file_r in enumerate(input_files_r):

    recid = targetids[idx]
    new_config_file = configdir + '/' + config_suff + '_%s'%(idx+1) + config_ext

    update_config_file(new_config_file, config_file, recid, sitecsv)
    config_files.append(new_config_file)

def get_subdir_from_ligid(ligid, layer_idx=1):
    """get subdirectory name"""
    minbin = (int(ligid[3:])-1)/nfolders_per_layer**layer_idx
    minbin = minbin*nfolders_per_layer**layer_idx + 1

    maxbin = min(minbin+nfolders_per_layer**layer_idx - 1, nligands)

    minbin_str = str(minbin)
    maxbin_str = str(maxbin)

    minbin_str = (len(ligid[3:])-len(minbin_str))*'0'+minbin_str
    maxbin_str = (len(ligid[3:])-len(maxbin_str))*'0'+maxbin_str
    return '/lig' + minbin_str + '-' + maxbin_str

if args.level in [0, 1]:
    scriptdir = 'to_submit_' + rundir
    shutil.rmtree(scriptdir, ignore_errors=True)
    os.mkdir(scriptdir)
    submit_all_filename = scriptdir + '/submit_all.sh'

ligand_idx_abs = 0
jobidx = 0
workdirs_script = []

for chunk in csv_iterator:
    input_files_l = [os.path.abspath(f) for f in list(chunk['file_origin'])]
    input_files_l_indices = list(chunk['index'])

    ligand_ids = list(chunk['ligID'])
    for jdx, ligid in enumerate(ligand_ids):
        ligand_idx_abs += 1
        subdir = rundir
        for layer_idx in range(nlayers, 0, -1):
            subdir += get_subdir_from_ligid(ligid, layer_idx=layer_idx)  

        for idx in range(ntargets):
            recid = targetids[idx]
            if use_target_folder:
                workdir = subdir + '/' + ligid + '/' + recid
            else:
                workdir = subdir + '/' + ligid
            if idx == 0:
                input_file_l_rel = os.path.relpath(input_files_l[jdx], start=workdir)
            if idx == 0 and jdx == 0:
                input_files_r_rel = []
                for file_r in input_files_r:
                    input_files_r_rel.append(os.path.relpath(file_r, start=workdir))
 
                config_files_rel = []
                for file_c in config_files:
                    config_files_rel.append(os.path.relpath(file_c, start=workdir))
            os.makedirs(workdir)

            script = ""
            if level == 0:
                filename = workdir + "/" + runscript_suffix + "." + scheduler
                script += queuing.make_header(scheduler_options, scheduler)
            else:
                filename = workdir + "/" + runscript_suffix + ".sh"
                script += '#!/bin/bash'
                script += "\n\n" + utils.get_babel_command(input_file_l_rel, index=input_files_l_indices[jdx])
                script += "\nrundbx -f %s -l ligand.mol2 -r %s"%(config_files_rel[idx], input_files_r_rel[idx])
            if rescore_only:
                script += " -rescore_only"
            script += "\nrm -rf ligand.mol2"

            with open(filename, 'w') as ff:
                ff.write(script)

        if level == 1:
            # update workdirs for scripts
            workdir_script = subdir + '/' + ligid
            if use_target_folder:
                workdir_script += '/target*' 
            workdirs_script.append(workdir_script)

            if ligand_idx_abs > 1 and (ligand_idx_abs%nligands_per_job == 0 or ligand_idx_abs == nligands):
                jobidx += 1 
                script = queuing.make_header(scheduler_options, scheduler, jobname="vs%s"%jobidx, output=scheduler+"-vs-%s.out"%jobidx, error=scheduler+"-vs-%s.err"%jobidx) 
                workdirs_script_str = ' '.join(workdirs_script)
                script += """\nset -e
dirs=`echo %(workdirs_script_str)s`
curdir=`pwd`
for dir in $dirs; do
  cd $dir
  bash %(runscript_suffix)s.sh
  cd $curdir
done\n"""%locals()
                with open(scriptdir+'/run_vs_%i.'%jobidx+scheduler, 'w') as ff:
                    ff.write(script)
                workdirs_script = []

if level == 0:
    dirs = rundir
    if use_subfolders:
        dirs += '/lig*'
        if use_subsubfolders:
            dirs += '/lig*'
    dirs += '/lig*'
    if use_target_folder:
        dirs += '/target*'
    if use_isomer_folder: 
        dirs += '/isomer*'
    line_iterate = "for dir in %s; do"%dirs
    # write script to submit all jobs
    script_all = """#!/bin/bash
set -e  
curdir=`pwd`
%(line_iterate)s
  cd $dir
  %(exe)s %(runscript_suffix)s.%(scheduler)s
  cd $curdir
done\n"""%locals()
    with open(submit_all_filename, 'w') as ff:
        ff.write(script_all)

elif level == 1:
    with open(submit_all_filename, 'w') as ff:
        ff.write("""#!/bin/bash
set -e
for file in %(scriptdir)s/run_vs_*.%(scheduler)s; do
  %(exe)s $file
done"""%locals())
